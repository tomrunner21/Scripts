{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinViz top gainer package preparations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19s8s2X301K779atIqxd5FUkBJ4xNeG75",
      "authorship_tag": "ABX9TyPOTZa7IScoY/TVZBTrp3TT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomrunner21/Scripts/blob/main/FinViz_top_gainer_package_preparations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3paBX-Nuf1u"
      },
      "source": [
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "import requests\n",
        "from requests import get\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import sleep\n",
        "\n",
        "import datetime\n",
        "from datetime import date\n",
        "import pytz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xi4lt7nvUZa"
      },
      "source": [
        "def page_amounts(url):\n",
        "  req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "  webpage = urlopen(req).read()\n",
        "  html = soup(webpage, \"html.parser\")\n",
        "  page_count = html.find_all(attrs = {'class': 'screener-pages'})\n",
        "\n",
        "  p_count = 1\n",
        "  for i in page_count:\n",
        "    p_count += 1\n",
        "  return p_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l37tU7UWBxMs"
      },
      "source": [
        "def pages(page_url, page_number):\n",
        "  top_gainers = []\n",
        "  pages = np.arange(1, ((page_number * 20) + 1), 20)\n",
        "  for page in pages:\n",
        "    url = (page_url +\"&r=\" + str(page))\n",
        "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    webpage = urlopen(req).read()\n",
        "    html = soup(webpage, \"html.parser\")\n",
        "\n",
        "    gainers = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "    for ticker in gainers:\n",
        "      top_gainers.append(ticker.get_text().strip())\n",
        "    sleep(2) #so we can get all pages every single time\n",
        "  return top_gainers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyFuxi3Bm4PI"
      },
      "source": [
        "utc_now = pytz.utc.localize(datetime.datetime.utcnow())\n",
        "today = utc_now.astimezone(pytz.timezone(\"America/Chicago\"))\n",
        "new_today = today.strftime(\"%m-%d-%Y\")\n",
        "filename = 'top_gainers_' + new_today + '.csv'\n",
        "\n",
        "top_gainers = []\n",
        "\n",
        "def get_top_gainers2(url, top_gainers):\n",
        "  p_count = page_amounts(url)\n",
        "  top_gainers = pages(url, p_count)\n",
        "  return top_gainers\n",
        "\n",
        "df = pd.DataFrame( get_top_gainers2(\"https://finviz.com/screener.ashx?v=110&s=ta_topgainers\", top_gainers) )\n",
        "#df\n",
        "df.to_csv(\"drive/My Drive/stocks daily top gainers/\" + filename)\n",
        "# df.to_csv(\"drive/My Drive/stocks daily top gainers/under_one \" + filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p-pWwiYqJxg"
      },
      "source": [
        "# def page_read(url):\n",
        "#   top_gainers = []\n",
        "#   req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "#   webpage = urlopen(req).read()\n",
        "#   html = soup(webpage, \"html.parser\")\n",
        "\n",
        "#   gainers = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "#   for ticker in gainers:\n",
        "#     top_gainers.append(ticker.get_text().strip())\n",
        "#   return top_gainers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl3B2kF_rSZd"
      },
      "source": [
        "# def multiple_pages(page_url, page_number):\n",
        "#   top_gainers = []\n",
        "#   pages = np.arange(21, ((page_number * 20) + 1), 20)\n",
        "#   for page in pages:\n",
        "#     url = (page_url +\"&r=\" + str(page))\n",
        "#     req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "#     webpage = urlopen(req).read()\n",
        "#     html = soup(webpage, \"html.parser\")\n",
        "\n",
        "#     gainers = html.find_all(attrs = {'class': 'screener-link-primary'})\n",
        "#     for ticker in gainers:\n",
        "#       top_gainers.append(ticker.get_text().strip())\n",
        "#     sleep(2) #so we can get all pages every single time\n",
        "#   return top_gainers"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}